<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Machine Learning</title>
    <meta charset="utf-8">
    <style>
     .left-column {
       width: 50%;
       float: left;
     }
     .right-column {
       width: 50%;
       float: right;
     }
     .grey { color: #bbbbbb; }
      
     .footnote {
        position: absolute;
        bottom: 3em;
        margin: 0em 2em;
      }

     .red {
       color: red;
      }

     .tiny { font-size: 0.75em; }
     .super-tiny { font-size: 0.70em; }

.pull-left {
    float: left;
    width: 45%;
}

.pull-right {
    float: right;
    width: 45%;
}
      </style>
    <link rel="stylesheet" type="text/css" href="slides.css">
  </head>
  <body>
      <textarea id="source">
class: center, middle

# Introduction to Machine Learning

Arthur Mensch

.affiliations[
![DeepMind](figures/dm.png)
]
.footnote.tiny[Credits: slides adapted from Mathieu Blondel (Google Brain), from the longer course we [gave](https://github.com/data-psl/lectures2020). Many contents and figures are borrowed from the scikit-learn [mooc](https://github.com/inria/scikit-learn-mooc/) and scipy
[lecture notes](https://scipy-lectures.org/packages/scikit-learn/index.html).]
---
# Scope of this lecturesl
- A one-hour and a half long tutorial for absolute beginners in machine learning
- High-level concepts with almost zero maths

<!-- --

What will be covered later this week but not today:
- Model-specific explanations (linear models, trees, neural networks)
- scikit-learn
- Optimization
- Deep learning -->

---
# Outline

- Supervised learning
 - Classification
 - Regression
 - Generalization

- Model selection
 - Model complexity
 - Overfitting
 - Bias / variance trade-off

---
class: center, middle

# Supervised learning
---
# Supervised learning

- Learning from examples (inductive learning)

- Training phase

.center[<img src="figures/training-phase.png" width="85%" />]

- Inference (prediction) phase

.center[<img src="figures/inference-phase.png" width="85%" />]

---
# Classification

- Given an input, predict the class (category) it belongs to 

- Examples
  - Given the picture of an astronomical object, determine whether it is a star, a galaxy, etc.
  - Given the picture of a person, identify the person
  - Given an email, detect whether it is a spam or not
  - Given the spectogram of a spoken word, recognize the command being spoken
---
# Regression

- Given an input, predict a real (continuous) variable associated with it

- Examples
 - Given the profile of a person, predict his/her salary
 - Given the characteristics of a house, predict its price
 - Given a patient's DNA information, predict his/her life expectancy
---
# The iris dataset

- A famous dataset in statistics and machine learning
.center[
<table>
  <tr>
    <td style="border-width: 0px">
      <img src="iris_setosa.jpg" style="width: 200px;" />
      <br />Setosa
    </td>
    <td style="border-width: 0px">
      <img src="iris_versicolor.jpg" style="width: 200px;" />
      <br />Versicolor
    </td>
    <td style="border-width: 0px">
      <img src="iris_virginica.jpg" style="width: 200px;" />
      <br />Viriginica
    </td>
  </tr>
</table>
]

.small[
| Sepal length | Sepal width | Petal length | Petal width | | Iris type  |
| ------------ | ----------- | ------------ | ----------- |-| ---------- |
| 6cm          | 3.4cm       | 4.5cm        | 1.6cm       | | versicolor |
| 5.7cm        | 3.8cm       | 1.7cm        | 0.3cm       | | setosa     |
| 6.5cm        | 3.2cm       | 5.1cm        | 2cm         | | virginica  |
| 5cm          | 3.cm        | 1.6cm        | 0.2cm       | | setosa     |
]
---
# Data matrix and target vector

$$
\mathbf{X} =
\begin{bmatrix}
\mathbf{x}_1 \\\\
\mathbf{x}_2 \\\\
\mathbf{x}_3 \\\\
\mathbf{x}_4 \\\\
\vdots 
\end{bmatrix}
=
\begin{bmatrix}
6 & 3.4 & 4.5 & 1.6 \\\\
5.7 & 3.8 & 1.7 & 0.3 \\\\
6.5 & 3.2 & 5.1 & 2 \\\\
5 & 3 & 1.6 & 0.2 \\\\
\vdots & \vdots & \vdots & \vdots
\end{bmatrix}
\quad
\mathbf{y}
=
\begin{bmatrix}
y_1 \\\\
y_2 \\\\
y_3 \\\\
y_4 \\\\
\vdots
\end{bmatrix}
=
\begin{bmatrix}
1 \\\\
0 \\\\
2 \\\\
0 \\\\
\vdots
\end{bmatrix}
$$

$\mathbf{X}$ is called the data matrix and consists of $n$ samples (observations) with $d$ features (attributes)

$\mathbf{x}_i$ is called a feature vector

$\mathbf{y}$ is called the target or label vector

$y_i$ is called a target or label
---
# Samples as points in a space

.center[
    <img src="sphx_glr_plot_iris_scatter_001.png" style="width: 500px;" />
]
Samples from the iris datasets live in a 4-dimensional space, 
we only use 2 dimensions for visualization purposes.
---
# Inferring rules from data

<table>
<thead><tr>
	<th>Sepal length</th>
	<th>Sepal width</th>
	<th>Petal length</th>
	<th>Petal width</th>
</tr></thead>
<tbody><tr>
    <td>
    <img src="figures/iris_sepal_length_cm_hist.svg" style="width: 100%;">
    </td>
    <td>
    <img src="figures/iris_sepal_width_cm_hist.svg" style="width: 100%;">
    </td>
    <td>
    <img src="figures/iris_petal_length_cm_hist.svg" style="width: 100%;">
    </td>
    <td>
    <img src="figures/iris_petal_width_cm_hist.svg" style="width: 100%;">
    </td>
    </tr></tbody>
</table>
<img src="figures/legend_irises.svg" style='float: left;'>

<br/><br/>We can *infer* from the data that setosa irises have small petals.

Learning algorithms automate this process!
---
# Learning a model from examples

- A model can be seen as a function $f$ that maps a feature vector $\mathbf{x}$ (input) to a target $y$ (output).

- Given input-output pairs (training examples) $(\mathbf{x}_1, y_1)$, $(\mathbf{x}_2, y_2)$,
..., $(\mathbf{x}_n, y_n)$, we want to find a $f$ such that

$$
f(\mathbf{x}_i) \approx y_i
$$

---
# Example: the nearest neighbor algorithm

$f(\mathbf{x})$ searches for the example $\mathbf{x}_i$ with smallest distance
$d(\mathbf{x}, \mathbf{x}_i)$ and returns the label $y_i$ associated with this example.

.center[<img src="sphx_glr_plot_iris_knn_001.png" width="65%"/>]

---
# US Census data

.super-tiny[

| Age | Workclass | Education    | Marital-status     | Occupation         | Relationship | Race  | Sex  | Capital-gain | Hours-per-week | Native-country | Class |
| --- | --------- | ------------ | ------------------ | ------------------ | ------------ | ----- | ---- | ------------ | -------------- | -------------- | ----- |
| 25  | Private   | 11th         | Never-married      | Machine-op-inspct  | Own-child    | Black | Male | 0            | 40             | United-States  | <=50K |
| 38  | Private   | HS-grad      | Married-civ-spouse | Farming-fishing    | Husband     | White  | Male | 0            | 50             | United-States   | <=50K |
| 28  | Local-gov | Assoc-acdm   | Married-civ-spouse | Protective-serv    | Husband      | White | Male | 0            | 40             | United-States   | >50K  |
| 44  | Private   | Some-college | Married-civ-spouse | Machine-op-inspct  | Husband      | Black | Male | 7688         | 40             | United-States   | >50K  |

]

Does this person make more than 50K per year?
---
# Generalizing

- In machine learning, we want to predict on new data instances.
That is, for a feature vector $\mathbf{x}$ not seen in the training set,
we want to build $f(\mathbf{x})$.

- In the census example, we want to predict the income of new
individuals, with a combination of jobs and demographics that we have
never seen.

- Many sources of variability: age workclass, education, marital-status,
occupation, ...

*+* Noise: unexplainable variance

---
# Memorizing

* Store all known individuals (the census)
* Given a new individual, predict the income of its closest match in our database
(nearest neighbor predictor)

Trying out this strategy on the data we have, the census, *what error
rate do we expect?*

--

> **0 errors**

--

.red[Yet, we will make errors on **new** data]

---
# Generalizing &nbsp;≠&nbsp; Memorizing

.center["test" data &nbsp;≠&nbsp; "train" data]

.pull-left[Data on which the predictive model is applied]

.pull-right[Data used to learn the predictive model]

&nbsp;   
&nbsp;   

* Different sampling of noise
* Unobserved combination of features
---
class: center, middle

# Model selection 
---
# Model complexity

- There exists a wealth a machine learning models, some more complex and expressive
than others.
- What are the implications of model complexity?
- How do we pick one model rather than another?

.center[
<table>
  <tr>
    <td style="border-width: 0px">
      <img src="sphx_glr_plot_svm_non_linear_001.png" style="width: 400px;" />
      <br/> Linear
    </td>
    <td style="border-width: 0px; padding: 0px">
      <img src="sphx_glr_plot_svm_non_linear_002.png" style="width: 400px;" />
      <br />Non-linear
    </td>
  </tr>
</table>
]
---
## Varying the degree of a polynomial

.center[<img src="figures/polynomial_overfit_0.svg" width=75%>]

.footnote.tiny[Reminder: a univariate polynomial of degree $D$ is given by
$f(x) \triangleq w_0 + w_1 x + w_2 x^2 + \dots + w_D x^D$.]
---
## Varying the degree of a polynomial

.center[<img src="figures/polynomial_overfit_1.svg" width=75%>]

.footnote.tiny[Reminder: a univariate polynomial of degree $D$ is given by
$f(x) \triangleq w_0 + w_1 x + w_2 x^2 + \dots + w_D x^D$.]
---
## Varying the degree of a polynomial

.center[<img src="figures/polynomial_overfit_2.svg" width=75%>]

.footnote.tiny[Reminder: a univariate polynomial of degree $D$ is given by
$f(x) \triangleq w_0 + w_1 x + w_2 x^2 + \dots + w_D x^D$.]
---
## Varying the degree of a polynomial

.center[<img src="figures/polynomial_overfit_5.svg" width=75%>]

.footnote.tiny[Reminder: a univariate polynomial of degree $D$ is given by
$f(x) \triangleq w_0 + w_1 x + w_2 x^2 + \dots + w_D x^D$.]
---
## Varying the degree of a polynomial

.center[<img src="figures/polynomial_overfit_9.svg" width=75%>]

.footnote.tiny[Reminder: a univariate polynomial of degree $D$ is given by
$f(x) \triangleq w_0 + w_1 x + w_2 x^2 + \dots + w_D x^D$.]
---
## Varying the degree of a polynomial

.center[<img src="figures/polynomial_overfit.svg" width=75%>]

.footnote.tiny[Reminder: a univariate polynomial of degree $D$ is given by
$f(x) \triangleq w_0 + w_1 x + w_2 x^2 + \dots + w_D x^D$.]
---
# Which model do you prefer?

.pull-left[<img src="figures/linear_ols.svg" width="110%">]

.pull-right[<img src="figures/linear_splines.svg" width="110%">]
---
# Which model do you prefer? (new data)

.pull-left[<img src="figures/linear_ols_test.svg" width="110%">]
.pull-right[<img src="figures/linear_splines_test.svg" width="110%">]

---
# Bias / Variance trade-off

.center[
    <img src="sphx_glr_plot_bias_variance_001.png" style="width: 800px;" />
]

- High bias: the model is too simple / constrained (underfit)
- High variance: the model is too expressive / flexible (overfit)
- Need to find a good trade-off
---
# Underfit versus overfit

.center[
<table>
  <tr>
    <td style="border-width: 0px">
<img src="figures/target_bias.svg" width="80%">
      <br/> High bias
    </td>
    <td style="border-width: 0px; padding: 0px">
<img src="figures/target_variance.svg" width="80%">
      <br />High variance
    </td>
  </tr>
</table>
]
---
# Train vs. test error

- Train set error: how well did the learning algorithm tune the model parameters

- Test set error: how well does the model generalize to new data
---
# Train vs test error: effect of model complexity

.pull-left[<img src="figures/polynomial_overfit_test_1.svg" width=110%>]
.pull-right[<img src="figures/polynomial_validation_curve_1.svg"
		width="100%">]
---
# Train vs test error: effect of model complexity

.pull-left[<img src="figures/polynomial_overfit_test_2.svg" width=110%>]
.pull-right[<img src="figures/polynomial_validation_curve_2.svg"
		width="100%">]
---
# Train vs test error: effect of model complexity

.pull-left[<img src="figures/polynomial_overfit_test_5.svg" width=110%>]
.pull-right[<img src="figures/polynomial_validation_curve_5.svg"
		width="100%">]
---
# Train vs test error: effect of model complexity

.pull-left[<img src="figures/polynomial_overfit_test_9.svg" width=110%>]
.pull-right[<img src="figures/polynomial_validation_curve_15.svg"
		width="100%">]
---
# Train vs test error: effect of sample size

.pull-left[<img
		       src="figures/polynomial_overfit_ntrain_42.svg"
		       width=110%>]
.pull-right[<img
		     src="figures/polynomial_learning_curve_42.svg"
		     width="100%">]
---
# Train vs test error: effect of sample size

.pull-left[<img
		       src="figures/polynomial_overfit_ntrain_145.svg"
		       width=110%>]
.pull-right[<img
		     src="figures/polynomial_learning_curve_145.svg"
		     width="100%">]
---
# Train vs test error: effect of sample size

.pull-left[<img
		       src="figures/polynomial_overfit_ntrain_1179.svg"
		       width=110%>]
.pull-right[<img
		     src="figures/polynomial_learning_curve_1179.svg"
		     width="100%">]
---
# Train vs test error: effect of sample size

.center[
<table>
  <tr>
    <td style="border-width: 0px">
<img src="figures/polynomial_overfit_ntrain_6766.svg" width=80%>
    </td>
    <td style="border-width: 0px; padding: 0px">
<img src="figures/polynomial_learning_curve_6766.svg" width="100%">
    </td>
  </tr>
</table>
]

.center[Diminishing returns phenomenon]
---
# Test vs. validation error

- Hyper-parameters: parameters that are not directly tuned by the learning algorithm
- Typically controls complexity (e.g. the degree parameter)
- Test data should never be used for choosing hyper-parameters
- Use validation data instead (aka held-out data)

.center[
    <img src="sphx_glr_plot_bias_variance_003.png" style="width: 500px;" />
]
---
# Cross-validation

- Repeatedly split the data into training and validation sets and average the accuracy scores

- More robust estimation of the quality of hyper-parameters, but requires to train the model several times

.center[
<img src="figures/cross_validation.png" width="75%" />
]

---
# Key concepts to always have in mind

* Machine learning is about extracting from *data* models that *generalize*
  to new observations.

* Bias (underfit) vs. Variance (overfit) trade-off.

* Never tune hyper-parameters / model complexity against the test data; 
use validation data or cross-validation instead.
---
# To go further

- Slides from our [longer course](https://github.com/data-psl/lectures2020/tree/master/slides)
- A notebook tutorial on `scikit-learn` (https://github.com/data-psl/lectures2020/tree/master/notebooks/02_sklearn) (can be done in a Colab)

    </textarea>
    <style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
		     all[i].SourceElement().parentNode.className += ' has-jax';
		     }
		     });
		     </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true
      });
    </script>
  </body>
</html>
